---
categories:  
- k8s    

tags:  
- kubesphere
---

# k8s 集群安装
![img.png](云原生实战/img.png)


## 准备3台云服务器
centos   
云服务器 规划到一个vpc(虚拟私有云网络) 中  
* master  172.31.0.2 
* worker1 172.31.0.3
* worker2 172.31.0.4

> master 上面可以装一个sshpass 用于快速连接其它两个worker

## 配置网络
教程中直接给3台服务器中绑定了3个公网ip，实际生产环境中往往只会配一个公网ip  
这里我们使用nat网关来实现，集群中任意一台机器都可以上网  
![img.png](云原生实战/img_1.png)  

[青云NAT网关配置文档](https://docsv3.qingcloud.com/network/nat_gateway/quickstart/snat_qs/)  

* SNAT  负责实现内网中 服务器访问公网
* DNAT  负责实现外网 访问内网服务器 （实现是类似端口转发）


## 3台服务器上面装 docker
### 配置yum源
```shell
sudo yum install -y yum-utils
sudo yum-config-manager \
--add-repo \
http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo

```
### 安装docker
```shell
sudo yum install -y docker-ce docker-ce-cli containerd.io


#以下是在安装k8s的时候使用
yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7  containerd.io-1.4.6
```


### 启动docker
```shell
systemctl enable docker --now
```
### 配置加速
```shell
sudo mkdir -p /etc/docker
sudo tee /etc/docker/daemon.json <<-'EOF'
{
  "registry-mirrors": ["https://82m9ar63.mirror.aliyuncs.com"],
  "exec-opts": ["native.cgroupdriver=systemd"],
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m"
  },
  "storage-driver": "overlay2"
}
EOF
sudo systemctl daemon-reload
sudo systemctl restart docker
```

## kubeadm创建集群

### 设置主机名（非必要）
推荐设置，设置后比较直观的能看到每台机器的角色  
在 master worker1 worker1 分别执行  
```shell
hostnamectl set-hostname k8s-master
hostnamectl set-hostname k8s-worker1
hostnamectl set-hostname k8s-worker2

```

### 基础环境
所有机器执行以下操作  (一下操作都是k8s官方要求做的) 
```shell

# 将 SELinux 设置为 permissive 模式（相当于将其禁用）
sudo setenforce 0
sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config

#关闭swap
swapoff -a  
sed -ri 's/.*swap.*/#&/' /etc/fstab

#允许 iptables 检查桥接流量
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
br_netfilter
EOF

cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sudo sysctl --system

```

### 安装kubelet、kubeadm、kubectl
```shell
cat <<EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=0
repo_gpgcheck=0
gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl
EOF


sudo yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes

sudo systemctl enable --now kubelet
```

### 使用kubeadm引导集群
#### 下载各个机器需要的镜像
```shell
sudo tee ./images.sh <<-'EOF'
#!/bin/bash
images=(
kube-apiserver:v1.20.9
kube-proxy:v1.20.9
kube-controller-manager:v1.20.9
kube-scheduler:v1.20.9
coredns:1.7.0
etcd:3.4.13-0
pause:3.2
)
for imageName in ${images[@]} ; do
docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
done
EOF
   
chmod +x ./images.sh && ./images.sh
```
#### 初始化主节点
```shell

#所有机器添加master域名映射，以下需要修改为自己的
echo "172.31.0.2  cluster-endpoint" >> /etc/hosts



#主节点初始化
kubeadm init \
--apiserver-advertise-address=172.31.0.2 \
--control-plane-endpoint=cluster-endpoint \
--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \
--kubernetes-version v1.20.9 \
--service-cidr=10.96.0.0/16 \
--pod-network-cidr=192.168.0.0/16

#所有网络范围不重叠

# 输出
Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join cluster-endpoint:6443 --token 2814mk.x9qrs7s68zgpvp4b \
    --discovery-token-ca-cert-hash sha256:5302308d2acb78fd732fa569daaeba074e0ca49006965d832e4eaa23f884dde7 \
    --control-plane 

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join cluster-endpoint:6443 --token 2814mk.x9qrs7s68zgpvp4b \
    --discovery-token-ca-cert-hash sha256:5302308d2acb78fd732fa569daaeba074e0ca49006965d832e4eaa23f884dde7

```

1. 指定配置文件  
```shell
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config
```
2. 安装网络组件
```shell
curl https://docs.projectcalico.org/archive/v3.20/manifests/calico.yaml -O

kubectl apply -f calico.yaml

```

#### 加入work节点
```shell
kubeadm join cluster-endpoint:6443 --token 2814mk.x9qrs7s68zgpvp4b \
    --discovery-token-ca-cert-hash sha256:5302308d2acb78fd732fa569daaeba074e0ca49006965d832e4eaa23f884dde7
```
> 这里注意 云服务器 我是先后创建的导致安全组用的不是一个，导致节点在加入集群的时候和master的6443端口的通信问题

> 新令牌
kubeadm token create --print-join-command


高可用部署方式，也是在这一步的时候，使用添加主节点的命令即可


####  验证集群 
```shell
kubectl get nodes 
```

## 部署dashboard
### 部署 
```shell
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml 
```


# 参考
https://www.yuque.com/leifengyang/oncloud/ctiwgo  

https://www.bilibili.com/video/BV13Q4y1C7hS?spm_id_from=333.337.search-card.all.click&vd_source=826416428f883a4b1c119869d5a0983b